# Database Performance Guide âš¡

<div dir="rtl">

## Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

Ø¯Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¬Ø¹Ù„ Queries Ø£Ø³Ø±Ø¹.

</div>

---

## ğŸ¯ 1. Indexing

### What is an Index?

<div dir="rtl">

**Index** Ù…Ø«Ù„ ÙÙ‡Ø±Ø³ Ø§Ù„ÙƒØªØ§Ø¨ - ÙŠØ³Ø§Ø¹Ø¯Ùƒ ØªØ¬Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø¯Ù„ Ù…Ø§ ØªÙ‚Ø±Ø£ Ø§Ù„ÙƒØªØ§Ø¨ ÙƒÙ„Ù‡.

</div>

### Without Index:

```sql
-- Search 1 million users
SELECT * FROM users WHERE email = 'ahmed@test.com';

â†’ Full table scan: checks ALL 1,000,000 rows ğŸ˜±
â†’ Time: ~500ms
```

### With Index:

```sql
-- Create index
CREATE INDEX idx_users_email ON users(email);

-- Same query now:
SELECT * FROM users WHERE email = 'ahmed@test.com';

â†’ Index lookup: finds row instantly âš¡
â†’ Time: ~2ms (250x faster!)
```

### When to Create Indexes:

```sql
-- âœ… Index columns used in WHERE
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_products_category ON products(category);

-- âœ… Index Foreign Keys
CREATE INDEX idx_orders_user_id ON orders(user_id);

-- âœ…Index columns used in ORDER BY
CREATE INDEX idx_products_created_at ON products(created_at);

-- âœ… Composite indexes for multiple columns
CREATE INDEX idx_products_category_price ON products(category, price);
```

### When NOT to Index:

```go
// âŒ Don't index:
// - Small tables (< 1000 rows)
// - Columns with low cardinality (e.g., boolean, status with few values)
// - Columns that change frequently
// - Rarely queried columns
```

---

## ğŸ” 2. Query Optimization

### Use EXPLAIN ANALYZE:

```sql
-- See query execution plan
EXPLAIN ANALYZE
SELECT * FROM products WHERE category = 'laptops';

-- Output shows:
-- - Seq Scan vs Index Scan
-- - Time taken
-- - Rows examined
```

### N+1 Query Problem:

```go
// âŒ BAD: N+1 queries
users := GetAllUsers()  // 1 query
for _, user := range users {
    user.Posts = GetUserPosts(user.ID)  // N queries!
}
// Total: 1 + N queries

// âœ… GOOD: Use JOIN or eager loading
SELECT users.*, posts.*
FROM users
LEFT JOIN posts ON posts.user_id = users.id;
// Only 1 query!

// In GORM:
db.Preload("Posts").Find(&users)  // 1 query
```

### Select Only Needed Columns:

```sql
-- âŒ BAD: Select everything
SELECT * FROM users;

-- âœ… GOOD: Select only needed
SELECT id, name, email FROM users;
```

### Use LIMIT:

```sql
-- âŒ BAD: Load everything
SELECT * FROM products;  -- 1 million rows!

-- âœ… GOOD: Paginate
SELECT * FROM products
ORDER BY created_at DESC
LIMIT 20 OFFSET 0;
```

---

## ğŸ’¾ 3. Connection Pooling

### The Problem:

```
Request 1: Open DB connection (100ms) + Query (2ms) = 102ms
Request 2: Open DB connection (100ms) + Query (2ms) = 102ms
Request 3: Open DB connection (100ms) + Query (2ms) = 102ms
...
```

### The Solution:

```go
import "database/sql"

db, _ := sql.Open("postgres", dsn)

// Configure pool
db.SetMaxOpenConns(25)          // Max 25 concurrent connections
db.SetMaxIdleConns(5)           // Keep 5 idle connections
db.SetConnMaxLifetime(5 * time.Minute)

// Now connections are reused!
// Request 1: Reuse connection + Query (2ms) = 2ms âš¡
```

---

## ğŸ—„ï¸ 4. Caching

### Application-Level Caching:

```go
import "github.com/go-redis/redis/v8"

func GetProduct(id int) (*Product, error) {
    // 1. Check cache first
    cacheKey := fmt.Sprintf("product:%d", id)
    cached, err := redisClient.Get(ctx, cacheKey).Result()

    if err == nil {
        // Cache HIT! âš¡
        var product Product
        json.Unmarshal([]byte(cached), &product)
        return &product, nil
    }

    // 2. Cache MISS - query database
    product, err := db.GetProduct(id)
    if err != nil {
        return nil, err
    }

    // 3. Store in cache
    jsonData, _ := json.Marshal(product)
    redisClient.Set(ctx, cacheKey, jsonData, 10*time.Minute)

    return product, nil
}
```

### Database Query Caching:

```sql
-- PostgreSQL automatically caches queries
-- But you can help by:
-- 1. Using prepared statements
-- 2. Consistent query patterns
```

---

## ğŸ“Š 5. Database Design

### Normalization:

```sql
-- âŒ BAD: Denormalized (data duplication)
CREATE TABLE orders (
    id INT,
    product_name VARCHAR,     -- Repeated!
    product_price DECIMAL,    -- Repeated!
    quantity INT
);

-- âœ… GOOD: Normalized
CREATE TABLE products (
    id INT PRIMARY KEY,
    name VARCHAR,
    price DECIMAL
);

CREATE TABLE orders (
    id INT PRIMARY KEY,
    product_id INT REFERENCES products(id),
    quantity INT
);
```

### Use Appropriate Data Types:

```sql
-- âŒ BAD: Wrong types
CREATE TABLE users (
    age VARCHAR(50),          -- Should be INT!
    price VARCHAR(100),       -- Should be DECIMAL!
    is_active VARCHAR(10)     -- Should be BOOLEAN!
);

-- âœ… GOOD: Correct types
CREATE TABLE users (
    age INT,
    price DECIMAL(10,2),
    is_active BOOLEAN
);
```

---

## ğŸ”„ 6. Batch Operations

### Instead of Multiple Inserts:

```go
// âŒ BAD: Insert one by one
for _, product := range products {
    db.Exec(
        "INSERT INTO products (name, price) VALUES ($1, $2)",
        product.Name,
        product.Price,
    )
}
// 100 products = 100 queries!

// âœ… GOOD: Batch insert
values := []interface{}{}
query := "INSERT INTO products (name, price) VALUES "
for i, product := range products {
    query += fmt.Sprintf("($%d, $%d)", i*2+1, i*2+2)
    if i < len(products)-1 {
        query += ", "
    }
    values = append(values, product.Name, product.Price)
}
db.Exec(query, values...)
// 100 products = 1 query! âš¡
```

---

## â±ï¸ 7. Slow Query Log

### Enable in PostgreSQL:

```sql
-- postgresql.conf
log_min_duration_statement = 1000  -- Log queries > 1 second

-- Check logs
SELECT * FROM pg_stat_statements
ORDER BY total_time DESC
LIMIT 10;
```

---

## ğŸ“ˆ 8. Monitoring

### Key Metrics to Track:

```go
// 1. Query execution time
// 2. Connection pool usage
// 3. Cache hit ratio
// 4. Slow queries
// 5. Database CPU/Memory
```

---

## âœ… Performance Checklist

<div dir="rtl">

- [ ] **Indexes** Ø¹Ù„Ù‰ columns Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ WHERE, ORDER BY
- [ ] **Foreign Keys** indexed
- [ ] **N+1 queries** eliminated (use JOINs/Preload)
- [ ] **SELECT \*** replaced with specific columns
- [ ] **LIMIT** used for large result sets
- [ ] **Connection pooling** configured
- [ ] **Caching** implemented (Redis/Memcached)
- [ ] **Batch operations** Ù„Ù„Ù€ bulk inserts/updates
- [ ] **EXPLAIN ANALYZE** Ù„Ù„Ù€ slow queries
- [ ] **Slow query log** enabled
- [ ] **Appropriate data types** used
- [ ] **Database normalized** (3NF usually)
- [ ] **Monitoring** in place

</div>

---

## ğŸ’¡ Quick Wins

<div dir="rtl">

### Ø£Ø³Ø±Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª:

1. **Add index** Ø¹Ù„Ù‰ email, foreign keys â†’ 10-100x faster
2. **Connection pooling** â†’ 50x faster
3. **Cache frequently accessed data** â†’ 100x faster
4. **Fix N+1 queries** â†’ 10x faster
5. **Use LIMIT** â†’ Don't load millions of rows

</div>

---

<div align="center">

[ğŸ“š Back to Module Home](../README.md)

</div>
